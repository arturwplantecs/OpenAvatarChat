"""
Pipeline service for AI processing
"""

import os
import sys
import asyncio
import yaml
import time
import base64
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging
import numpy as np

# Add the project src to path to import original handlers
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

logger = logging.getLogger(__name__)


class PipelineService:
    """Service that manages the AI processing pipeline"""
    
    def __init__(self):
        self.config = {}
        self.handlers = {}
        self.is_initialized = False
        self.initialization_time = None
        self.session_manager = None  # Will be injected from main.py
        
        # Handler instances
        self.vad_handler = None
        self.asr_handler = None
        self.llm_handler = None
        self.tts_handler = None
        self.avatar_handler = None
        
        # Processing statistics
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "avg_processing_time": 0.0
        }
    
    async def initialize(self):
        """Initialize the pipeline with handlers"""
        try:
            logger.info("ðŸ”§ Initializing AI pipeline...")
            start_time = time.time()
            
            # Load configuration
            await self._load_config()
            
            # Initialize handlers
            await self._initialize_handlers()
            
            self.is_initialized = True
            self.initialization_time = time.time() - start_time
            
            logger.info(f"âœ… Pipeline initialized in {self.initialization_time:.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ Pipeline initialization failed: {e}")
            raise
    
    async def _load_config(self):
        """Load pipeline configuration"""
        try:
            # First try to load main_config.yaml from API directory
            api_config_path = Path(__file__).parent.parent / "main_config.yaml"
            
            if api_config_path.exists():
                config_path = api_config_path
            else:
                # Fallback to original config
                config_path = project_root / "config" / "chat_with_faster_whisper_stable.yaml"
            
            if not config_path.exists():
                raise FileNotFoundError(f"Config file not found: {config_path}")
            
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            
            logger.info(f"ðŸ“‹ Loaded config from {config_path}")
            
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            # Use fallback config
            self.config = self._get_fallback_config()
            logger.warning("Using fallback configuration")
    
    def _get_fallback_config(self) -> Dict[str, Any]:
        """Get fallback configuration if config file is not available"""
        return {
            "vad": {
                "name": "silero_vad",
                "model_path": "models/silero_vad.onnx"
            },
            "asr": {
                "name": "faster_whisper",
                "model_path": "models/faster-whisper-large-v3",
                "language": "pl",
                "cpu_threads": 4
            },
            "llm": {
                "name": "openai_compatible",
                "api_base": "http://localhost:11434/v1",
                "model_name": "qwen2.5:7b",
                "max_tokens": 150,
                "temperature": 0.7
            },
            "tts": {
                "name": "piper",
                "model_path": "models/piper/pl_mls_piper-medium.onnx",
                "speaker_id": 0
            },
            "avatar": {
                "name": "lite_avatar",
                "model_path": "models/lite_avatar",
                "fps": 25
            }
        }
    
    async def _initialize_handlers(self):
        """Initialize all pipeline handlers"""
        try:
            # Try to initialize real handlers first, fallback to mock
            if await self._try_initialize_real_handlers():
                logger.info("âœ… Real handlers initialized successfully")
            else:
                await self._initialize_mock_handlers()
                logger.info("âš ï¸  Using mock handlers - install dependencies for real functionality")
            
        except Exception as e:
            logger.error(f"Handler initialization failed: {e}")
            raise
    
    async def _try_initialize_real_handlers(self) -> bool:
        """Try to initialize real handlers, return True if successful"""
        try:
            # Initialize real ASR handler with FasterWhisper
            await self._initialize_real_asr()
            
            # Initialize real TTS handler with PiperTTS
            await self._initialize_real_tts()
            
            # Initialize real Avatar handler with LiteAvatar
            await self._initialize_real_avatar()
            
            # Initialize real LLM handler
            await self._initialize_real_llm()
            
            return True
            
        except Exception as e:
            logger.warning(f"Failed to initialize real handlers: {e}")
            return False
    
    async def _initialize_real_asr(self):
        """Initialize real ASR handler with FasterWhisper"""
        try:
            from faster_whisper import WhisperModel
            
            # Access the correct config structure
            chat_engine_config = self.config.get("default", {}).get("chat_engine", {})
            asr_config = chat_engine_config.get("handler_configs", {}).get("FasterWhisper", {})
            
            model_path = asr_config.get("model_size", "large-v3")
            device = asr_config.get("device", "cpu")
            compute_type = asr_config.get("compute_type", "int8")
            language = asr_config.get("language", "pl")
            
            class FasterWhisperHandler:
                def __init__(self, model_path, device, compute_type, language):
                    self.model = WhisperModel(model_path, device=device, compute_type=compute_type)
                    self.language = language
                    logger.info(f"FasterWhisper model loaded: {model_path}")
                
                async def process_async(self, audio_data: bytes):
                    import tempfile
                    import os
                    
                    # Save audio to temporary file
                    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                        temp_file.write(audio_data)
                        temp_file_path = temp_file.name
                    
                    try:
                        # Run transcription in executor
                        loop = asyncio.get_event_loop()
                        segments, info = await loop.run_in_executor(
                            None, 
                            self._transcribe_file, 
                            temp_file_path
                        )
                        
                        text = " ".join([segment.text for segment in segments]).strip()
                        return {"text": text}
                        
                    finally:
                        if os.path.exists(temp_file_path):
                            os.unlink(temp_file_path)
                
                def _transcribe_file(self, file_path):
                    return self.model.transcribe(
                        file_path,
                        language=self.language,
                        vad_filter=True,
                        vad_parameters=dict(min_silence_duration_ms=500)
                    )
            
            self.asr_handler = FasterWhisperHandler(model_path, device, compute_type, language)
            logger.info("âœ… Real FasterWhisper ASR handler initialized")
            
        except ImportError:
            logger.warning("faster-whisper not available")
            raise
        except Exception as e:
            logger.error(f"Failed to initialize FasterWhisper: {e}")
            raise
    
    async def _initialize_real_tts(self):
        """Initialize real TTS handler with PiperTTS"""
        try:
            import subprocess
            import tempfile
            import librosa
            import numpy as np
            
            # Check if PiperTTS model exists
            chat_engine_config = self.config.get("default", {}).get("chat_engine", {})
            tts_config = chat_engine_config.get("handler_configs", {}).get("PiperTTS", {})
            model_path = tts_config.get("model_path", "models/piper/pl_PL-gosia-medium.onnx")
            
            # Look for the model in API directory first, then project root
            model_full_path = None
            for base_path in [Path(__file__).parent.parent, project_root]:
                potential_path = base_path / model_path
                if potential_path.exists():
                    model_full_path = potential_path
                    break
            
            if not model_full_path:
                logger.warning(f"PiperTTS model not found: {model_path}")
                raise FileNotFoundError(f"PiperTTS model not found: {model_path}")
            
            # Find piper executable
            piper_executable = None
            try:
                result = subprocess.run(['which', 'piper'], capture_output=True, text=True)
                if result.returncode == 0:
                    piper_executable = result.stdout.strip()
                else:
                    # Try common paths
                    common_paths = ['/usr/local/bin/piper', '/usr/bin/piper', 'piper']
                    for path in common_paths:
                        try:
                            result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=2)
                            if result.returncode == 0:
                                piper_executable = path
                                break
                        except:
                            continue
            except:
                pass
            
            if not piper_executable:
                logger.warning("Piper executable not found, falling back to mock TTS")
                raise FileNotFoundError("Piper executable not found")
            
            class PiperTTSHandler:
                def __init__(self, model_path, executable):
                    self.model_path = model_path
                    self.executable = executable
                    self.sample_rate = tts_config.get("sample_rate", 24000)
                    self.length_scale = tts_config.get("length_scale", 1.0)
                    self.noise_scale = tts_config.get("noise_scale", 0.1)
                    self.noise_w = tts_config.get("noise_w", 0.1)
                    logger.info(f"PiperTTS initialized: {model_path}")
                
                async def process_async(self, text: str):
                    try:
                        # Create temporary file for output
                        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                            temp_path = temp_file.name
                        
                        try:
                            # Prepare Piper command
                            cmd = [
                                self.executable,
                                '--model', str(self.model_path),
                                '--output_file', temp_path,
                            ]
                            
                            # Add parameters
                            if self.length_scale != 1.0:
                                cmd.extend(['--length_scale', str(self.length_scale)])
                            if self.noise_scale != 0.667:
                                cmd.extend(['--noise_scale', str(self.noise_scale)])
                            if self.noise_w != 0.8:
                                cmd.extend(['--noise_w', str(self.noise_w)])
                            
                            # Run Piper synthesis
                            process = await asyncio.create_subprocess_exec(
                                *cmd,
                                stdin=asyncio.subprocess.PIPE,
                                stdout=asyncio.subprocess.PIPE,
                                stderr=asyncio.subprocess.PIPE
                            )
                            
                            stdout, stderr = await asyncio.wait_for(
                                process.communicate(input=text.encode('utf-8')),
                                timeout=5.0
                            )
                            
                            if process.returncode != 0:
                                logger.error(f"Piper failed: {stderr.decode()}")
                                raise RuntimeError(f"Piper synthesis failed")
                            
                            # Load and encode audio
                            audio_data, _ = librosa.load(temp_path, sr=self.sample_rate, mono=True)
                            
                            # Convert to 16-bit PCM and encode
                            audio_16bit = (audio_data * 32767).astype(np.int16)
                            audio_bytes = audio_16bit.tobytes()
                            
                            return {
                                "audio_data": base64.b64encode(audio_bytes).decode('utf-8'),
                                "sample_rate": self.sample_rate,
                                "format": "pcm_s16le"
                            }
                            
                        finally:
                            try:
                                import os
                                os.unlink(temp_path)
                            except:
                                pass
                            
                    except asyncio.TimeoutError:
                        logger.error(f"PiperTTS synthesis timeout for: {text[:50]}...")
                        # Return silence
                        silence = np.zeros(int(self.sample_rate * 0.5), dtype=np.int16)
                        return {
                            "audio_data": base64.b64encode(silence.tobytes()).decode('utf-8'),
                            "sample_rate": self.sample_rate,
                            "format": "pcm_s16le"
                        }
                    except Exception as e:
                        logger.error(f"PiperTTS synthesis error: {e}")
                        # Return silence  
                        silence = np.zeros(int(self.sample_rate * 0.5), dtype=np.int16)
                        return {
                            "audio_data": base64.b64encode(silence.tobytes()).decode('utf-8'),
                            "sample_rate": self.sample_rate,
                            "format": "pcm_s16le"
                        }
            
            self.tts_handler = PiperTTSHandler(model_full_path, piper_executable)
            logger.info("âœ… Real PiperTTS handler initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize PiperTTS: {e}")
            raise
    
    async def _initialize_real_avatar(self):
        """Initialize real Avatar handler with LiteAvatar"""
        try:
            # Check if LiteAvatar models exist
            chat_engine_config = self.config.get("default", {}).get("chat_engine", {})
            avatar_config = chat_engine_config.get("handler_configs", {}).get("LiteAvatar", {})
            avatar_name = avatar_config.get("avatar_name", "20250408/P1-hDQRxa5xfpZK-1yDX8PrQ")
            
            # Look for LiteAvatar models in API directory first, then fallback to project root
            api_avatar_path = Path(__file__).parent.parent / "resource" / "avatar" / "liteavatar" / avatar_name
            project_avatar_path = project_root / "resource" / "avatar" / "liteavatar" / avatar_name
            
            avatar_models_exist = False
            avatar_model_path = None
            
            # Check API directory first
            if api_avatar_path.exists() and (api_avatar_path / "net.pth").exists():
                avatar_models_exist = True
                avatar_model_path = api_avatar_path
                logger.info(f"Found avatar models in API directory: {api_avatar_path}")
            # Then check project root
            elif project_avatar_path.exists() and (project_avatar_path / "net.pth").exists():
                avatar_models_exist = True
                avatar_model_path = project_avatar_path
                logger.info(f"Found avatar models in project directory: {project_avatar_path}")
            
            if not avatar_models_exist:
                logger.warning(f"LiteAvatar models not found for avatar: {avatar_name}")
                await self._initialize_mock_avatar()
                return
            
            class LiteAvatarHandler:
                def __init__(self, model_path, avatar_name, config):
                    self.model_path = model_path
                    self.avatar_name = avatar_name
                    self.fps = config.get("fps", 25)
                    self.enable_fast_mode = config.get("enable_fast_mode", True)
                    self.use_gpu = config.get("use_gpu", True)
                    self.debug = config.get("debug", False)
                    self.lite_avatar = None
                    self.is_initialized = False
                    
                    logger.info(f"LiteAvatar initialized: {avatar_name} at {model_path}")
                    logger.info(f"LiteAvatar settings: fps={self.fps}, fast_mode={self.enable_fast_mode}, gpu={self.use_gpu}")
                    
                    # Initialize the avatar neural networks
                    self._initialize_avatar()
                
                def _initialize_avatar(self):
                    """Initialize the LiteAvatar neural network"""
                    try:
                        # Import the LiteAvatar class
                        import sys
                        import os
                        algo_path = Path(__file__).parent.parent / "handlers" / "avatar" / "liteavatar" / "algo" / "liteavatar"
                        if not algo_path.exists():
                            # Fallback to src directory
                            project_root = Path(__file__).parent.parent.parent
                            algo_path = project_root / "src" / "handlers" / "avatar" / "liteavatar" / "algo" / "liteavatar"
                        
                        # Change to the algo directory (required for relative paths in LiteAvatar)
                        original_cwd = os.getcwd()
                        sys.path.insert(0, str(algo_path))
                        os.chdir(str(algo_path))
                        
                        logger.info(f"Changed working directory to: {os.getcwd()}")
                        
                        from lite_avatar import liteAvatar
                        
                        # Initialize with the model data directory
                        self.lite_avatar = liteAvatar(
                            data_dir=str(self.model_path),
                            fps=self.fps,
                            use_gpu=self.use_gpu
                        )
                        
                        # Load the dynamic model (neural networks)
                        self.lite_avatar.load_dynamic_model(str(self.model_path))
                        self.is_initialized = True
                        
                        # Change back to original directory
                        os.chdir(original_cwd)
                        
                        logger.info(f"âœ… LiteAvatar neural network loaded successfully from {self.model_path}")
                        
                    except Exception as e:
                        logger.error(f"âŒ Failed to initialize LiteAvatar: {e}")
                        logger.exception("Full traceback:")
                        # Change back to original directory on error
                        if 'original_cwd' in locals():
                            os.chdir(original_cwd)
                        self.is_initialized = False
                
                async def process_async(self, text: str, audio_data: str):
                    try:
                        if not self.is_initialized or not self.lite_avatar:
                            logger.warning("LiteAvatar not initialized, generating idle frames")
                            return await self._generate_idle_frames()
                        
                        # Decode base64 audio
                        import base64
                        try:
                            audio_bytes = base64.b64decode(audio_data)
                        except:
                            logger.error("Failed to decode audio data")
                            return await self._generate_idle_frames()
                        
                        # Process audio through neural network to get facial parameters
                        param_res = self.lite_avatar.audio2param(
                            input_audio_byte=audio_bytes,
                            prefix_padding_size=0,
                            is_complete=True
                        )
                        
                        video_frames = []
                        total_bg_frames = len(self.lite_avatar.ref_img_list)
                        
                        # Continue background frame cycling from where idle left off
                        # Use a persistent background counter to avoid resets
                        if not hasattr(self, 'bg_frame_counter'):
                            self.bg_frame_counter = 0
                            self.bg_direction = 1  # 1 for forward, -1 for backward
                        
                        # Generate video frames from parameters with continuous background
                        for i, param in enumerate(param_res):
                            # Use continuous background frame cycling (no reset)
                            bg_frame_id = self.bg_frame_counter % total_bg_frames
                            
                            # Update background counter with smooth progression
                            # Slower movement during speech for more natural look
                            if i % 3 == 0:  # Update background every 3 frames for smoother transition
                                self.bg_frame_counter += self.bg_direction
                                if self.bg_frame_counter >= total_bg_frames - 1:
                                    self.bg_direction = -1
                                    self.bg_frame_counter = total_bg_frames - 1
                                elif self.bg_frame_counter <= 0:
                                    self.bg_direction = 1
                                    self.bg_frame_counter = 0
                            
                            # Generate mouth image from neural network
                            mouth_img = self.lite_avatar.param2img(param, bg_frame_id)
                            
                            # Merge mouth with background to create full avatar frame
                            full_img, _ = self.lite_avatar.merge_mouth_to_bg(mouth_img, bg_frame_id)
                            
                            # Convert frame to base64
                            import cv2
                            import base64
                            _, buffer = cv2.imencode('.jpg', full_img, [cv2.IMWRITE_JPEG_QUALITY, 85])
                            frame_b64 = base64.b64encode(buffer).decode('utf-8')
                            video_frames.append(frame_b64)
                        
                        logger.info(f"âœ… Generated {len(video_frames)} real avatar frames for text: {text[:50]}...")
                        return {"video_frames": video_frames}
                        
                    except Exception as e:
                        logger.error(f"âŒ LiteAvatar processing error: {e}")
                        logger.exception("Full traceback:")
                        return await self._generate_idle_frames()
                
                async def _generate_idle_frames(self, num_frames: int = 30):
                    """Generate idle avatar frames when no audio or as fallback"""
                    if not self.is_initialized or not self.lite_avatar:
                        return {"video_frames": []}
                    
                    try:
                        video_frames = []
                        idle_param = self.lite_avatar.get_idle_param()  # Neutral facial expression
                        total_bg_frames = len(self.lite_avatar.ref_img_list)
                        
                        # Initialize or continue background frame counter
                        if not hasattr(self, 'bg_frame_counter'):
                            self.bg_frame_counter = 0
                            self.bg_direction = 1  # 1 for forward, -1 for backward
                        
                        # Generate idle frames with continuous background cycling
                        for i in range(num_frames):
                            # Use the same background counter as speech for continuity
                            bg_frame_id = self.bg_frame_counter % total_bg_frames
                            
                            # Update counter with ping-pong motion (same as speech)
                            self.bg_frame_counter += self.bg_direction
                            if self.bg_frame_counter >= total_bg_frames - 1:
                                self.bg_direction = -1
                                self.bg_frame_counter = total_bg_frames - 1
                            elif self.bg_frame_counter <= 0:
                                self.bg_direction = 1
                                self.bg_frame_counter = 0
                            
                            # Generate idle frame with neutral expression but varying background
                            mouth_img = self.lite_avatar.param2img(idle_param, bg_frame_id)
                            full_img, _ = self.lite_avatar.merge_mouth_to_bg(mouth_img, bg_frame_id)
                            
                            # Convert to base64
                            import cv2
                            import base64
                            _, buffer = cv2.imencode('.jpg', full_img, [cv2.IMWRITE_JPEG_QUALITY, 90])
                            frame_b64 = base64.b64encode(buffer).decode('utf-8')
                            video_frames.append(frame_b64)
                        
                        logger.info(f"âœ… Generated {len(video_frames)} natural idle avatar frames")
                        return {"video_frames": video_frames}
                        
                    except Exception as e:
                        logger.error(f"âŒ Error generating idle frames: {e}")
                        return {"video_frames": []}
            
            self.avatar_handler = LiteAvatarHandler(avatar_model_path, avatar_name, avatar_config)
            logger.info("âœ… Real LiteAvatar handler initialized with model files")
            
        except Exception as e:
            logger.error(f"Failed to initialize LiteAvatar: {e}")
            await self._initialize_mock_avatar()
    
    async def _initialize_mock_avatar(self):
        """Initialize mock avatar handler"""
        class MockAvatarHandler:
            async def process_async(self, text: str, audio_data: str):
                # Return empty video frames - no avatar display
                return {"video_frames": []}
        
        self.avatar_handler = MockAvatarHandler()
        logger.info("âœ… Mock Avatar handler initialized")
    
    async def _initialize_real_llm(self):
        """Initialize real LLM handler using OpenAI-compatible API"""
        try:
            # Access the correct config structure with 'default' root key
            chat_engine_config = self.config.get("default", {}).get("chat_engine", {})
            llm_config = chat_engine_config.get("handler_configs", {}).get("LLM_Bailian", {})
            
            if not llm_config.get("enabled", False):
                logger.warning("LLM handler is disabled in config")
                await self._initialize_mock_llm()
                return
            
            # Create a simplified LLM handler wrapper
            class OpenAILLMHandler:
                def __init__(self, config):
                    self.model_name = config.get("model_name", "gpt-4o-mini")
                    self.system_prompt = config.get("system_prompt", "You are a helpful assistant. Always respond in Polish.")
                    self.api_key = config.get("api_key")
                    self.api_url = config.get("api_url", "https://api.openai.com/v1")
                    
                    if not self.api_key:
                        raise ValueError("OpenAI API key is required")
                    
                    logger.info(f"LLM Config: model={self.model_name}, api_url={self.api_url}")
                    
                async def process_async(self, text: str, session_id: str = None, conversation_history: List[Dict[str, str]] = None):
                    try:
                        # Import OpenAI client
                        from openai import OpenAI
                        
                        client = OpenAI(
                            api_key=self.api_key,
                            base_url=self.api_url
                        )
                        
                        # Create messages with conversation history
                        messages = [{"role": "system", "content": self.system_prompt}]
                        
                        # Add conversation history if provided
                        if conversation_history:
                            messages.extend(conversation_history)
                            logger.info(f"ðŸ’¬ Using conversation history: {len(conversation_history)} previous messages")
                        
                        # Add current user message
                        messages.append({"role": "user", "content": text})
                        
                        logger.info(f"ðŸ§  Sending to OpenAI: {self.model_name} - '{text[:50]}...' (total messages: {len(messages)})")
                        
                        # Call OpenAI API
                        response = client.chat.completions.create(
                            model=self.model_name,
                            messages=messages,
                            temperature=0.7,
                            max_tokens=500
                        )
                        
                        response_text = response.choices[0].message.content
                        logger.info(f"OpenAI response: {response_text[:100]}...")
                        
                        return {"text": response_text}
                        
                    except Exception as e:
                        logger.error(f"OpenAI API call failed: {e}")
                        # Fallback to mock response
                        return {"text": "Przepraszam, wystÄ…piÅ‚ problem z poÅ‚Ä…czeniem. SprÃ³buj ponownie."}
            
            self.llm_handler = OpenAILLMHandler(llm_config)
            logger.info("âœ… Real OpenAI LLM handler initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize real LLM: {e}")
            logger.info("Falling back to mock LLM")
            await self._initialize_mock_llm()

    async def _initialize_mock_llm(self):
        """Initialize mock LLM handler"""
        
        class MockLLMHandler:
            async def process_async(self, text: str, session_id: str = None):
                # Mock LLM response
                responses = [
                    "DziÄ™kujÄ™ za twoje pytanie. Jak mogÄ™ ci pomÃ³c?",
                    "To bardzo interesujÄ…ce zagadnienie. Co chciaÅ‚byÅ› wiedzieÄ‡ wiÄ™cej?",
                    "Rozumiem twÃ³j punkt widzenia. Czy masz jakieÅ› dodatkowe pytania?",
                    "Åšwietnie! Jestem tutaj, aby ci pomÃ³c w kaÅ¼dej sprawie.",
                    "To dobra obserwacja. Czy chcesz, Å¼ebym rozwinÄ…Å‚ ten temat?"
                ]
                import random
                response = random.choice(responses)
                return {"text": response}
        
        self.llm_handler = MockLLMHandler()
        logger.info("âœ… Mock LLM handler initialized")
    
    async def _initialize_mock_handlers(self):
        """Initialize mock handlers for testing purposes"""
        
        class MockHandler:
            def __init__(self, name):
                self.name = name
                
            async def process_async(self, data, *args):
                # Simulate processing time
                await asyncio.sleep(0.1)
                return self._mock_process(data, *args)
                
            def process(self, data, *args):
                return self._mock_process(data, *args)
                
            def _mock_process(self, data, *args):
                if self.name == "vad":
                    return {"has_speech": True}
                elif self.name == "asr":
                    return {"text": "PrzykÅ‚adowy transkrybowany tekst."}
                elif self.name == "llm":
                    return {"text": "To jest przykÅ‚adowa odpowiedÅº od asystenta AI."}
                elif self.name == "tts":
                    # Return mock base64 encoded audio
                    mock_audio = b"mock_audio_data"
                    return {"audio_data": base64.b64encode(mock_audio).decode('utf-8')}
                elif self.name == "avatar":
                    # Return mock video frames
                    mock_frame = b"mock_video_frame"
                    return {"video_frames": [base64.b64encode(mock_frame).decode('utf-8')]}
                
            async def initialize(self, config):
                pass
                
            async def cleanup(self):
                pass
        
        # Initialize mock handlers
        self.vad_handler = MockHandler("vad")
        self.asr_handler = MockHandler("asr") 
        self.llm_handler = MockHandler("llm")
        self.tts_handler = MockHandler("tts")
        self.avatar_handler = MockHandler("avatar")
        
        logger.info("âœ… Mock handlers initialized (replace with real handlers)")
    
    async def _initialize_real_handlers(self):
        """Initialize real handlers - to be implemented when handlers are properly imported"""
        # TODO: Replace mock handlers with real ones
        # This method will contain the real handler initialization
        # when the import issues are resolved
        pass
    
    async def process_audio(self, audio_data: bytes, session_id: str) -> Dict[str, Any]:
        """Process audio input through the pipeline"""
        if not self.is_initialized:
            raise RuntimeError("Pipeline not initialized")
        
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            result = {}
            
            # Step 1: Voice Activity Detection (optional)
            if self.vad_handler:
                vad_result = await self._run_vad(audio_data)
                if not vad_result.get("has_speech", True):
                    return {"error": "No speech detected"}
                result["vad"] = vad_result
            
            # Step 2: Speech Recognition
            asr_result = await self._run_asr(audio_data)
            transcribed_text = asr_result.get("text", "")
            if not transcribed_text.strip():
                return {"error": "No text transcribed"}
            
            result["asr"] = asr_result
            result["transcribed_text"] = transcribed_text
            
            # Step 3: Generate response with LLM
            llm_result = await self._run_llm(transcribed_text, session_id)
            response_text = llm_result.get("text", "")
            result["llm"] = llm_result
            result["response_text"] = response_text
            
            # Step 4: Text-to-Speech
            if response_text:
                tts_result = await self._run_tts(response_text)
                result["tts"] = tts_result
                result["audio_data"] = tts_result.get("audio_data")
                
                # Step 5: Avatar generation (optional)
                if self.avatar_handler and tts_result.get("audio_data"):
                    avatar_result = await self._run_avatar(response_text, tts_result.get("audio_data"))
                    result["avatar"] = avatar_result
                    result["video_frames"] = avatar_result.get("video_frames", [])
            
            # Update stats
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            self._update_stats(processing_time, success=True)
            
            return result
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"Audio processing failed: {e}")
            raise
    
    async def process_text(self, text: str, session_id: str) -> Dict[str, Any]:
        """Process text input through the pipeline"""
        if not self.is_initialized:
            raise RuntimeError("Pipeline not initialized")
        
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            result = {}
            result["input_text"] = text
            
            # Step 1: Generate response with LLM
            llm_result = await self._run_llm(text, session_id)
            response_text = llm_result.get("text", "")
            result["llm"] = llm_result
            result["response_text"] = response_text
            
            # Step 2: Text-to-Speech
            if response_text:
                tts_result = await self._run_tts(response_text)
                result["tts"] = tts_result
                result["audio_data"] = tts_result.get("audio_data")
                
                # Step 3: Avatar generation (optional)
                if self.avatar_handler and tts_result.get("audio_data"):
                    avatar_result = await self._run_avatar(response_text, tts_result.get("audio_data"))
                    result["avatar"] = avatar_result
                    result["video_frames"] = avatar_result.get("video_frames", [])
            
            # Update stats
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            self._update_stats(processing_time, success=True)
            
            return result
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"Text processing failed: {e}")
            raise
    
    async def _run_vad(self, audio_data: bytes) -> Dict[str, Any]:
        """Run Voice Activity Detection"""
        try:
            if hasattr(self.vad_handler, 'process_async'):
                return await self.vad_handler.process_async(audio_data)
            else:
                # Fallback to sync method
                return self.vad_handler.process(audio_data)
        except Exception as e:
            logger.error(f"VAD processing failed: {e}")
            return {"has_speech": True}  # Default to having speech
    
    async def _run_asr(self, audio_data: bytes) -> Dict[str, Any]:
        """Run Automatic Speech Recognition"""
        try:
            if hasattr(self.asr_handler, 'process_async'):
                return await self.asr_handler.process_async(audio_data)
            else:
                # Run in thread pool for sync methods
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self.asr_handler.process, audio_data)
        except Exception as e:
            logger.error(f"ASR processing failed: {e}")
            raise
    
    async def _run_llm(self, text: str, session_id: str) -> Dict[str, Any]:
        """Run Large Language Model with conversation history"""
        try:
            # Add user message to session history
            if self.session_manager:
                await self.session_manager.add_message_to_history(
                    session_id, "user", {"text": text}
                )
            
            # Get conversation history for context
            conversation_history = []
            if self.session_manager:
                history = await self.session_manager.get_conversation_history(session_id, limit=10)
                # Convert session history to LLM format
                for msg in history[:-1]:  # Exclude the current message we just added
                    content = msg.get("content", {})
                    if msg.get("type") == "user":
                        conversation_history.append({
                            "role": "user",
                            "content": content.get("text", "")
                        })
                    elif msg.get("type") == "assistant":
                        conversation_history.append({
                            "role": "assistant", 
                            "content": content.get("text", "")
                        })
            
            # Process with LLM (pass conversation history if supported)
            if hasattr(self.llm_handler, 'process_async'):
                result = await self.llm_handler.process_async(text, session_id, conversation_history)
            else:
                # Fallback for handlers that don't support history
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, self.llm_handler.process, text)
            
            # Add assistant response to session history
            if self.session_manager and "response_text" in result:
                await self.session_manager.add_message_to_history(
                    session_id, "assistant", {"text": result["response_text"]}
                )
            
            return result
        except Exception as e:
            logger.error(f"LLM processing failed: {e}")
            raise
    
    async def _run_tts(self, text: str) -> Dict[str, Any]:
        """Run Text-to-Speech"""
        try:
            if hasattr(self.tts_handler, 'process_async'):
                result = await self.tts_handler.process_async(text)
            else:
                # Run in thread pool for sync methods
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, self.tts_handler.process, text)
            
            # Ensure audio data is base64 encoded
            if "audio_data" in result and isinstance(result["audio_data"], bytes):
                result["audio_data"] = base64.b64encode(result["audio_data"]).decode('utf-8')
            
            return result
        except Exception as e:
            logger.error(f"TTS processing failed: {e}")
            raise
    
    async def _run_avatar(self, text: str, audio_data: str) -> Dict[str, Any]:
        """Run Avatar generation"""
        try:
            if hasattr(self.avatar_handler, 'process_async'):
                result = await self.avatar_handler.process_async(text, audio_data)
            else:
                # Run in thread pool for sync methods
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, self.avatar_handler.process, text, audio_data)
            
            # Ensure video frames are base64 encoded
            if "video_frames" in result:
                frames = result["video_frames"]
                if frames and isinstance(frames[0], bytes):
                    result["video_frames"] = [base64.b64encode(frame).decode('utf-8') for frame in frames]
            
            return result
        except Exception as e:
            logger.error(f"Avatar processing failed: {e}")
            return {"video_frames": []}
    
    def _update_stats(self, processing_time: float, success: bool):
        """Update processing statistics"""
        if success:
            self.stats["successful_requests"] += 1
            
            # Update average processing time
            total_successful = self.stats["successful_requests"]
            current_avg = self.stats["avg_processing_time"]
            self.stats["avg_processing_time"] = ((current_avg * (total_successful - 1)) + processing_time) / total_successful
    
    async def generate_idle_frames(self, num_frames: int = 30) -> Dict[str, List[str]]:
        """Generate idle avatar frames for continuous playback"""
        try:
            if not self.avatar_handler:
                logger.warning("Avatar handler not available for idle frames")
                return {"video_frames": []}
            
            logger.info(f"ðŸŽ­ Generating {num_frames} idle avatar frames...")
            
            # Check if avatar handler has _generate_idle_frames method
            if hasattr(self.avatar_handler, '_generate_idle_frames'):
                result = await self.avatar_handler._generate_idle_frames(num_frames)
                logger.info(f"âœ… Generated {len(result.get('video_frames', []))} idle frames")
                return result
            else:
                # Fallback: use regular processing with empty text
                result = await self._process_avatar("", "")
                
                # If we got frames, repeat them to reach desired count
                frames = result.get("video_frames", [])
                if frames:
                    # Repeat frames to reach desired count
                    repeated_frames = []
                    for i in range(num_frames):
                        repeated_frames.append(frames[i % len(frames)])
                    logger.info(f"âœ… Generated {len(repeated_frames)} idle frames (repeated)")
                    return {"video_frames": repeated_frames}
                else:
                    logger.warning("No frames generated for idle animation")
                    return {"video_frames": []}
                    
        except Exception as e:
            logger.error(f"âŒ Failed to generate idle frames: {e}")
            return {"video_frames": []}
    
    async def get_status(self) -> Dict[str, Any]:
        """Get pipeline status"""
        return {
            "initialized": self.is_initialized,
            "vad_available": self.vad_handler is not None,
            "asr_available": self.asr_handler is not None,
            "llm_available": self.llm_handler is not None,
            "tts_available": self.tts_handler is not None,
            "avatar_available": self.avatar_handler is not None
        }
    
    async def get_detailed_status(self) -> Dict[str, Any]:
        """Get detailed pipeline status"""
        component_status = {}
        
        # Check each component
        for name, handler in [
            ("vad", self.vad_handler),
            ("asr", self.asr_handler), 
            ("llm", self.llm_handler),
            ("tts", self.tts_handler),
            ("avatar", self.avatar_handler)
        ]:
            if handler is not None:
                component_status[name] = {
                    "status": "healthy",
                    "available": True,
                    "config": self.config.get(name, {})
                }
            else:
                component_status[name] = {
                    "status": "unavailable",
                    "available": False
                }
        
        # Determine overall status
        critical_components = ["asr", "llm", "tts"]
        critical_healthy = all(
            component_status.get(comp, {}).get("available", False) 
            for comp in critical_components
        )
        
        if critical_healthy:
            overall_status = "healthy"
        else:
            overall_status = "unhealthy"
        
        return {
            "overall_status": overall_status,
            "components": component_status,
            "stats": self.stats,
            "initialization_time": self.initialization_time,
            "uptime": time.time() - (self.initialization_time or time.time()) if self.initialization_time else 0
        }
    
    async def cleanup(self):
        """Cleanup pipeline resources"""
        logger.info("ðŸ§¹ Cleaning up pipeline...")
        
        # Cleanup handlers
        for handler in [self.vad_handler, self.asr_handler, self.llm_handler, self.tts_handler, self.avatar_handler]:
            if handler and hasattr(handler, 'cleanup'):
                try:
                    await handler.cleanup()
                except Exception as e:
                    logger.error(f"Handler cleanup error: {e}")
        
        self.is_initialized = False
        logger.info("âœ… Pipeline cleanup completed")
